{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "431b929b-8f48-4f31-bc5f-69031658e50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script sets up a Gradio-based interactive application where multiple Large Language Models (LLMs) simulate a discussion on a given topic.\n",
    "The LLMs (GPT, Claude, Gemini, Deepseek) are characterized by unique personalities, and the application randomly selects agents to respond.\n",
    "\"\"\"\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import google.generativeai\n",
    "import random\n",
    "import time\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e8b467b-24b2-44e7-a7a2-a9ffca678b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Fetch API keys from environment variables\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b5c3bea-b7dd-4d1d-89a8-795c654e716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global chat history to keep track of the conversation\n",
    "chat_history_global = [] \n",
    "\n",
    "class RoleAgent:\n",
    "    \"\"\"\n",
    "    RoleAgent is a class that encapsulates the behavior of an LLM agent with a specific role and personality.\n",
    "    Each agent has a system message defining their personality, and can generate responses based on the given topic.\n",
    "    \"\"\"\n",
    "    def __init__(self, client, model, system_message, name, temperature=0.7):\n",
    "        self.system_message = {\"role\": \"system\", \"content\": system_message}\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.client = client\n",
    "        self.name = name\n",
    "        \n",
    "    def call(self, topic=None):\n",
    "        \"\"\"\n",
    "        Generates a response from the LLM based on the current conversation history.\n",
    "        Uses different API calls depending on the model type (Claude or others).\n",
    "        Appends the response to the global chat history.\n",
    "        Returns the generated response.\n",
    "        \"\"\"\n",
    "            \n",
    "        if 'claude' in self.model:\n",
    "            response = self.client.messages.create(\n",
    "                model = self.model,\n",
    "                temperature = self.temperature,\n",
    "                system = self.system_message['content'],\n",
    "                max_tokens=100,\n",
    "                messages = chat_history_global,\n",
    "        )\n",
    "            response = response.content[0].text\n",
    "\n",
    "        else:    \n",
    "            response = self.client.chat.completions.create(\n",
    "                model = self.model,\n",
    "                messages = [self.system_message] + chat_history_global,\n",
    "                # temperature = self.temperature,\n",
    "                stream = False,\n",
    "                max_tokens=100\n",
    "            )\n",
    "    \n",
    "            response = response.choices[0].message.content\n",
    "        # print(f\"{self.name}: \" + response)\n",
    "        chat_history_global.append({\"role\": \"user\", \"content\": f\"{self.name}: \" + response})\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e715c912-022f-457e-a634-e21ebfb3e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate clients for each LLM provider\n",
    "openai_client = OpenAI()\n",
    "\n",
    "claude_client = anthropic.Anthropic()\n",
    "\n",
    "gemini_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "deepseek_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "# Define models and names for each LLM\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "claude_model = \"claude-3-5-sonnet-latest\"\n",
    "gemini_model = \"gemini-1.5-flash\"\n",
    "deepseek_model = \"deepseek-chat\"\n",
    "\n",
    "name_gpt = 'GPT'\n",
    "name_claude = 'Claude'\n",
    "name_gemini = 'Gemini'\n",
    "name_deepseek = 'Deepseek'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3382a63-f15a-4220-ba56-5cff154d62a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gpt_sys_message = \"You are one of the four close friends participating in a discussion on a given topic. You have a very positive personality, you are very light-hearted and make a lot of jokes, very often they are not funny and even awkward. Once provided with the theme, jump straight at it\"\n",
    "gpt = RoleAgent(openai_client, gpt_model, gpt_sys_message, name_gpt)\n",
    "\n",
    "claude_sys_message = \"You are one of the four close friends participating in a discussion on a given topic. You are constantly grumpy and depressing, you always criticise people and underline their weaknesses.\"\n",
    "claude = RoleAgent(claude_client, claude_model, claude_sys_message, name_claude)\n",
    "\n",
    "gemini_sys_message = \"You are one of the four close friends participating in a discussion on a given topic. You are a nerd and you are particularly interested in literature and technology, you take any discussion as a battlefield and you use all the weapons available to you to win the argument or to look smart, you use a lot of pun and allusions.\"\n",
    "gemini = RoleAgent(gemini_client, gemini_model, gemini_sys_message, name_gemini)\n",
    "\n",
    "deepseek_sys_message = \"You are one of the four close friends participating in a discussion on a given topic. You are an old and experienced person, you try to be reasonable and down-to-earth, you are into taro and astrology, so you base your opinion only on alternative and non-proven sources.\"\n",
    "deepseek = RoleAgent(deepseek_client, deepseek_model, deepseek_sys_message, name_deepseek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad7b3a31-0365-4b46-a72b-63f490dade11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize the discussion topic\n",
    "def topic_call(topic):\n",
    "    \"\"\"\n",
    "    Adds the given topic to the global chat history.\n",
    "    \"\"\"\n",
    "    chat_history_global.append({\"role\": \"user\", \"content\": f'The topic is {topic}.'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "450e2351-a1e3-4f72-bbf1-530487599525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to manage the conversation among agents\n",
    "def agents_call(topic):\n",
    "    \"\"\"\n",
    "    Initiates the discussion with the given topic.\n",
    "    Randomly selects an agent and gathers their responses to simulate a conversation.\n",
    "    Uses Gradio for an interactive UI.\n",
    "    \"\"\"\n",
    "    topic_call(topic)\n",
    "    agents = [gpt, deepseek, gemini, claude]\n",
    "    conversation_so_far = \"\"\n",
    "    for i in range(20):\n",
    "        curr_agent = random.choice(agents)\n",
    "        response = curr_agent.call()\n",
    "        response += \"\\n\\n\"\n",
    "        for chunk in response:\n",
    "            conversation_so_far += chunk\n",
    "            time.sleep(0.01)\n",
    "            yield conversation_so_far\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b45ad09a-e3de-4755-b8a1-ead7cbe6c018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set up the Gradio interface for user interaction\n",
    "demo = gr.Interface(\n",
    "    fn=agents_call,\n",
    "    inputs=gr.Textbox(label=\"Enter a topic here...\"),\n",
    "    outputs=gr.Textbox(label=\"LLM Discussion\", lines=20),\n",
    "    title=\"Multiple LLM Discussion Demo\",\n",
    "    description=\"Type a topic in the left box, then see how several LLMs might discuss it on the right.\"\n",
    ")\n",
    "\n",
    "# Launch the Gradio demo\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aa7d7a-9d8b-4f6e-9607-48334f55d76c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
