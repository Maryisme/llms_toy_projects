{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70668917-d74d-4f80-bfaf-7e28258235d6",
   "metadata": {},
   "source": [
    "# Overview of the \"Company Brochure Generator\" Code\n",
    "\n",
    "This code is designed to automatically scrape a company's website, identify relevant sub-pages (such as About, Careers, or other important sections), and compile the scraped information into a concise *markdown* brochure. It uses **Large Language Models (LLMs)**—either GPT or Claude—to determine which links are relevant and to produce the final brochure text.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "1. **Imports and Configuration**  \n",
    "   - Loads libraries like `requests` and `BeautifulSoup` for web scraping.  \n",
    "   - Utilizes environment variables (via `dotenv`) for API keys, such as OpenAI, Claude, and Google’s PaLM-based generative AI.  \n",
    "   - Sets up basic request headers to resemble a standard web browser.\n",
    "\n",
    "2. **`Website` Class**  \n",
    "   - Represents a single webpage.  \n",
    "   - Fetches the page, parses out the `<title>`, removes non-text elements (scripts, style, images, etc.), and stores the cleaned-up textual content.  \n",
    "   - Collects all hyperlinks (`<a>` tags) for later analysis.\n",
    "\n",
    "3. **LLM Prompts and Link Handling**  \n",
    "   - Includes predefined system instructions (`link_system_prompt`) that instruct the LLM to filter links relevant to a brochure.  \n",
    "   - The `get_links_user_prompt` function prepares the user prompt listing all discovered links.  \n",
    "   - `get_links(url, model)` then uses either GPT or Claude to parse those links and return a JSON list of relevant URLs (e.g., About pages, Careers pages, etc.).\n",
    "\n",
    "4. **Gathering Additional Page Content**  \n",
    "   - The `get_all_details(url, model)` function first retrieves textual content from the main (landing) page.  \n",
    "   - It calls `get_links` to identify sub-pages (like About, Careers), then fetches each page’s text, accumulating everything into a single string.\n",
    "\n",
    "5. **Creating the Brochure**  \n",
    "   - A separate system prompt (`system_prompt`) tells the LLM how to structure the final brochure, focusing on company information, culture, customers, and career opportunities.  \n",
    "   - The `get_brochure_user_prompt(url, model)` function compiles the landing page content and relevant sub-page content, truncating if it’s over 5,000 characters.  \n",
    "   - Finally, `create_brochure(url, model)` sends that text, along with the system prompt, to GPT or Claude, receiving back a short, markdown-formatted brochure.\n",
    "\n",
    "6. **Gradio Web Interface**  \n",
    "   - The last part of the code defines a Gradio `Interface` with two inputs:\n",
    "     1. A textbox for the **company website URL**.  \n",
    "     2. A dropdown for selecting the **LLM model** (GPT or Claude).  \n",
    "   - The **output** is a markdown text block that displays the generated brochure to the user.\n",
    "\n",
    "## Why This Code Is Useful\n",
    "\n",
    "- **Automates Website Content Gathering**  \n",
    "  Instead of manually copying and pasting a company’s About/Careers sections, the script programmatically collects data from the main webpage and relevant linked pages.\n",
    "\n",
    "- **Leverages Large Language Models**  \n",
    "  By employing GPT or Claude, the code transforms raw text into a *coherent, formatted brochure*, potentially saving a lot of manual editing work.\n",
    "\n",
    "- **Flexible and Configurable**  \n",
    "  - It can be adapted to different websites or even different LLMs.  \n",
    "  - The user can quickly switch between GPT and Claude to see which model yields better (or more cost-effective) results.\n",
    "\n",
    "- **Easy-to-Use Web App**  \n",
    "  Thanks to Gradio, anyone (technical or not) can enter a company site URL and get an instant summary/brochure, without having to run Python scripts in a CLI.\n",
    "\n",
    "Overall, this code serves as a powerful demonstration of how to combine **web scraping** with **text-generation AI** to produce dynamic, context-rich marketing or informational materials.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c44c5494-950d-4d2f-8d4f-b87b57c5b330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import google.generativeai\n",
    "import anthropic\n",
    "import gradio as gr \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "337d5dfc-0181-4e3b-8ab9-e78e0c3f657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22586021-1795-4929-8079-63f5bb4edd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic and Google\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "claude = anthropic.Anthropic()\n",
    "\n",
    "google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "396812b4-efb0-461c-94e6-5a838c4b69be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define request headers to mimic a typical browser\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "\n",
    "# Define a Website class for scraping and storing webpage data\n",
    "class Website:\n",
    "    \"\"\"\n",
    "    A utility class to represent a Website that we have scraped with links\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        self.body = response.content\n",
    "        soup = BeautifulSoup(self.body, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        if soup.body:\n",
    "            for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "                irrelevant.decompose()\n",
    "            self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
    "        else:\n",
    "            self.text = \"\"\n",
    "        links = [link.get('href') for link in soup.find_all('a')]\n",
    "        self.links = [link for link in links if link]\n",
    "\n",
    "    def get_contents(self):\n",
    "        return f\"Webpage Title:\\n{self.title}\\nWebpage Contents:\\n{self.text}\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dedfdecc-10d8-41df-acc1-6df9cddd4fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define system prompts and user prompts for link handling\n",
    "\n",
    "link_system_prompt = \"You are provided with a list of links found on a webpage. \\\n",
    "You are able to decide which of the links would be most relevant to include in a brochure about the company, \\\n",
    "such as links to an About page, or a Company page, or Careers/Jobs pages.\\n\"\n",
    "link_system_prompt += \"You should respond in JSON as in this example:\"\n",
    "link_system_prompt += \"\"\"\n",
    "{\n",
    "    \"links\": [\n",
    "        {\"type\": \"about page\", \"url\": \"https://full.url/goes/here/about\"},\n",
    "        {\"type\": \"careers page\": \"url\": \"https://another.full.url/careers\"}\n",
    "    ]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a08e8d5-cedd-470f-8e1c-ac8bea8087f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links_user_prompt(website):\n",
    "    \"\"\"\n",
    "    Creates a user prompt string that lists all links for the given website.\n",
    "    Asks the assistant to identify only those relevant for a company brochure.\n",
    "    \"\"\"\n",
    "    user_prompt = f\"Here is the list of links on the website of {website.url} - \"\n",
    "    user_prompt += \"please decide which of these are relevant web links for a brochure about the company, respond with the full https URL in JSON format. \\\n",
    "Do not include Terms of Service, Privacy, email links.\\n\"\n",
    "    user_prompt += \"Links (some might be relative links):\\n\"\n",
    "    user_prompt += \"\\n\".join(website.links)\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "891a2c42-1468-4f36-b7f0-5e673d41e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve relevant links from a given URL using either GPT or Claude models\n",
    "\n",
    "def get_links(url, model):\n",
    "    \"\"\"\n",
    "    Fetches the webpage at 'url', scrapes the links, and uses a chosen LLM model\n",
    "    to classify which links are relevant for a company brochure.\n",
    "\n",
    "    :param url: Website URL to scrape\n",
    "    :param model: 'GPT' or 'Claude' to indicate which LLM to use\n",
    "    :return: JSON-like Python dict containing relevant links\n",
    "    \"\"\"\n",
    "    website = Website(url)\n",
    "    if model == 'GPT':\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": link_system_prompt},\n",
    "                {\"role\": \"user\", \"content\": get_links_user_prompt(website)}\n",
    "          ],\n",
    "            response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "    else:\n",
    "        result = claude.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=1000,\n",
    "        temperature=0.7,\n",
    "        system=link_system_prompt + 'Please return the answer in valid JSON format only. Do not include any extra text or explanations outside the JSON.',\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": get_links_user_prompt(website)},\n",
    "        ]\n",
    "    )\n",
    "        return json.loads(result.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10edbb8b-5166-435f-adf0-4fb643645666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to gather details from the landing page and its relevant sub-pages\n",
    "\n",
    "def get_all_details(url, model):\n",
    "    \"\"\"\n",
    "    Scrapes the main landing page (given by 'url') and also scrapes any relevant\n",
    "    linked pages, as determined by the chosen LLM model. Returns a concatenated\n",
    "    string of the textual content from all these pages.\n",
    "\n",
    "    :param url: Website URL to scrape\n",
    "    :param model: 'GPT' or 'Claude' for link classification\n",
    "    :return: A formatted string with the contents of the landing page and relevant sub-pages\n",
    "    \"\"\"\n",
    "    result = \"Landing page:\\n\"\n",
    "    result += Website(url).get_contents()\n",
    "    links = get_links(url, model)\n",
    "    # print(\"Found links:\", links)\n",
    "    for link in links[\"links\"]:\n",
    "        result += f\"\\n\\n{link['type']}\\n\"\n",
    "        result += Website(link[\"url\"]).get_contents()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55fe7992-d2aa-4a63-8c3f-f8f0a9c10d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a system prompt for generating the brochure text\n",
    "\n",
    "system_prompt = \"You are an assistant that analyzes the contents of several relevant pages from a company website \\\n",
    "and creates a short brochure about the company for prospective customers, investors and recruits. Respond in markdown.\\\n",
    "Include details of company culture, customers and careers/jobs if you have the information.\"\n",
    "\n",
    "# Or uncomment the lines below for a more humorous brochure - this demonstrates how easy it is to incorporate 'tone':\n",
    "\n",
    "# system_prompt = \"You are an assistant that analyzes the contents of several relevant pages from a company website \\\n",
    "# and creates a short humorous, entertaining, jokey brochure about the company for prospective customers, investors and recruits. Respond in markdown.\\\n",
    "# Include details of company culture, customers and careers/jobs if you have the information.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "312a1bd3-190c-4621-b5f9-f4ee8a2b91ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build the user prompt used in the final brochure creation step\n",
    "\n",
    "def get_brochure_user_prompt(url, model):\n",
    "    \"\"\"\n",
    "    Constructs a user prompt by pulling all relevant details from the website (landing + sub-pages)\n",
    "    and instructing the model to create a short brochure in markdown format.\n",
    "\n",
    "    :param url: Main website URL\n",
    "    :param model: 'GPT' or 'Claude'\n",
    "    :return: A truncated string (max 5000 chars) containing relevant textual content\n",
    "    \"\"\"\n",
    "    user_prompt = f\"You are looking at a company webpage:\\n\"\n",
    "    user_prompt += f\"Here are the contents of its landing page and other relevant pages; use this information to build a short brochure of the company in markdown.\\n\"\n",
    "    user_prompt += get_all_details(url, model)\n",
    "    user_prompt = user_prompt[:5_000] # Truncate if more than 5,000 characters\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d92e33b-bc6d-49e0-a4a2-78273f15d51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to create the brochure text using either GPT or Claude\n",
    "\n",
    "def create_brochure(url, model):\n",
    "    \"\"\"\n",
    "    Uses the combined text from 'get_brochure_user_prompt' and a pre-defined 'system_prompt'\n",
    "    to generate a short company brochure in markdown.\n",
    "\n",
    "    :param url: Company website URL\n",
    "    :param model: 'GPT' or 'Claude'\n",
    "    :return: The generated markdown brochure\n",
    "    \"\"\"\n",
    "    if model == 'GPT':\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": get_brochure_user_prompt(url, model)}\n",
    "              ],\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    else:\n",
    "        result = claude.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=2000,\n",
    "        temperature=0.7,\n",
    "        system= system_prompt,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": get_brochure_user_prompt(url, model)},\n",
    "        ],\n",
    "    )\n",
    "        return result.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8210f304-efc2-4afa-9918-37f65a2ae2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradio interface to launch the web UI\n",
    "# This lets users input a URL and select a model (GPT or Claude)\n",
    "# and displays the generated brochure in markdown.\n",
    "\n",
    "view = gr.Interface(\n",
    "    fn=create_brochure,\n",
    "    inputs=[gr.Textbox(label=\"Company website:\"), gr.Dropdown([\"GPT\", \"Claude\"], label=\"Select model\", value=\"GPT\")],\n",
    "    outputs=[gr.Markdown(label=\"Brochure:\")],\n",
    "    flagging_mode=\"never\"\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92efd496-1c23-4f93-825b-1c871a64c892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
